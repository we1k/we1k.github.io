<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="mix 7 mix 8">
<meta property="og:type" content="website">
<meta property="og:title" content="Notes&amp;Words">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Notes&amp;Words">
<meta property="og:description" content="mix 7 mix 8">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="we1k">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Notes&Words</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Notes&Words</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/12/%E7%88%B1%E6%83%85%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="we1k">
      <meta itemprop="description" content="mix 7 mix 8">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Notes&Words">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/12/%E7%88%B1%E6%83%85%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">爱情笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-02-12 11:20:04 / Modified: 23:24:13" itemprop="dateCreated datePublished" datetime="2022-02-12T11:20:04+08:00">2022-02-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>搭配音乐，效果更佳哦！</p>
</blockquote>
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/5YAuUz0Nagt9QxYheiQ9zk?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture">
</iframe>
<h3 id="对过去的回忆">对过去的回忆</h3>
<h4 id="高中的初识">高中的初识</h4>
<p>红墙绿瓦之中的石室中学，春秋几度的更替带来了一届届懵懂岁月的少年们。三年的时光让他们相遇、相识、相知。时光把发生在这里的故事封存在每个秋天金黄的银杏叶里，若能翻开沫若园前的青石板，曾经的那段往事将再次回到眼前。</p>
<p>2015年的银杏叶飘落的时候，这便是我们故事的开始....</p>
<p><img src="https://i.imgur.com/02WNTYd.jpg" title="石室中学" /></p>
<h5 id="生根">生根</h5>
<p>一个女孩和一个少年踌躇满志的踏进这片校园里，从陌生人到同班同学是命运为他们彼此间换上的标签。</p>
<p>仍记得刚上高一的自己，胸中满怀孤高的傲气。经常口出狂言是对自己自命不凡却又无足轻重宿命的抗争，但是换来的只有同学间的冷嘲热讽。四处碰壁的磨难并没有让他学到谦卑，反而令他的心之壁更加坚固。直到那一天，那个女孩迈着轻柔的脚步，小心翼翼的接近那座高耸的城堡，才踏入一片荒芜的内心。</p>
<p>每月班上照例的团建是少年最讨厌的活动了。这次例会的活动是“老鹰捉小鸡”，同学们在你追我赶的嬉戏中发出阵阵欢笑。而少年两手揣在兜里，像一个孤魂，一边漫无目的散步，一边一脸冷漠的望着这群“无趣”的人。突然女孩带着一群人将他团团围住，向他表明他已经被“捉住”，根据游戏规则，少年应该加入他们的队伍一起寻找下个目标。少年保持着满脸的不屑，对女孩说道：“这种游戏挺没意思的，我不会玩的”。正当其他同学心里自觉无趣，正准备离开时，女孩忽然间牵起男孩的左手，满脸笑容的对着男孩，一句话也没说，开始带着他一起奔跑。女生心中的温柔伴随着手上余热的体温，一下间冲到男孩心中那片永冻冰中，霎那间冰层融化，万物复苏，那股热烈的温柔幻化为一粒种子，第一次播撒在这贫瘠荒芜的废土上。而从这里开始，种子也就此开始生根发芽~~</p>
<h5 id="萌芽">萌芽</h5>
<p>晚自习上传递的字条，那是两个少年们此生共同的回忆。</p>
<p><img src="https://i.imgur.com/dDphJa7.jpg?3" title="source: imgur.com" /></p>
<p><em>["基你个大头鬼，哈狗"],</em></p>
<p>手机上敲下的01字符，被转成银河中<strong>Everlasting</strong>(此生不渝的)电磁波信号，不断向整个宇宙广播着他们的恋爱故事。</p>
<p><img src="https://i.imgur.com/OyBDWDT.jpg" title="永恒的热恋信息" /></p>
<p><em>[你知道吗? 如果能够把手机上无线电波放慢接近2000倍，我们就能再次用“<u>看见</u>”那段时间晚上我们互发的消息啦 : ) ]</em></p>
<p>两个悸动的心灵，两个不完美的圆，开始逐渐交融在一起，不断的，不断的融合成为一个<strong>完美</strong>的圆。</p>
<p><a target="_blank" rel="noopener" href="https://imgur.com/HURVhN8"><img src="https://i.imgur.com/HURVhN8.gif" title="两个心" /></a></p>
<p><em>[[]<sub>(￣▽￣)</sub> 我用我指尖 画心型的圈, 然后碎碎念 想象你听见]</em></p>
<h5 id="歧途">歧途</h5>
<blockquote>
<p>搭配音乐，效果更佳哦！ <iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2gL8ZVsX4YydhkRzktur6L?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe></p>
</blockquote>
<p>记得那一次，大学里和朋友玩真心话大冒险时<em>(那是第一次，也是唯一一次)</em>，朋友问我："你最难忘的时光是哪一段时间？"我不假思索的说：“2016年的那个夏天....”</p>
<p>我知道，这对你来说是非常难过的一段时光，我竟然如此自私，残忍，狠下心让你受尽了委屈。所以请先允许我向你致上最真诚的抱歉。真的很对不起你！我不该太过自私！！！！</p>
<p>那整个暑假的晚安，其实每句我都有看到。甚至那段时间我一度形成了一种条件反射式的渴求，迫切需要那句"晚安"来对每天的生活画上最后一个句点。我相信你现在会懂得，我当时说我不关心，说我不难过，那是我内心的懦弱罢了。</p>
<p>你如此爱我至深，甚至让我袒露我的脆弱！如今想起来为什么我的眼泪会流啊，我的心会痛啊？</p>
<p>我人生中的第一次失眠，就是从那时开始的....</p>
<blockquote>
<p>我在每一个夜晚</p>
<p>和每一个月亮</p>
<p>诉说着同一个你</p>
<p>《失眠》</p>
</blockquote>
<p>我当时竟然痴痴地认为，只要忍过这个暑假，你自然会选择放手。所以我需要做的，就是忍一天，再忍下一天，直到你不会给我发晚安为止。可是一个16岁少女的心中，究竟是拥有怎样海枯石烂的精神力量才会支撑起我的整个青春。</p>
<p>现在，我终于想明白一个道理，忍耐不是真正的解脱，面对现实才会！！！</p>
<p><a target="_blank" rel="noopener" href="https://imgur.com/uxOUEul"><img src="https://i.imgur.com/uxOUEul.jpg" title="source: imgur.com" style="zoom:50%;" /></a></p>
<p><em>[早知如此，<u>何必当初.</u>...]</em></p>
<h5 id="冰冻">冰冻！</h5>
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7yriqF8s3ESXqwuDxUaleo?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture">
</iframe>
<blockquote>
<p>清微道长说：真正爱一个人，就要放手 ---《仙剑奇侠传3》</p>
<p>我说 ： 狗屁！！！！！！！ --- lzw</p>
</blockquote>
<p>后面的事情你应该也都知道了，暑假的最后一天，我认为是时候该下定决心了，既然你内心如此强大，那我就只能忍痛割爱--删掉你的联系方式了。(这真的是我干出来的事啊？！我真的抱歉)。不久之后也因为座位的变动我们最终分开，而你也确实没再来找过我。我以为这就是故事的结束，原来开在荒野上的花朵终究抵不过凛冽的寒风...（难道我那时候还在期望着什么吗？现在回想起来真的挺搞笑的）</p>
<p>但是后来的一段时间，我发现我整个人都不在状态了，像丢了魂。明明上课盯向黑板的眼睛会突然失去焦点，再次回过神来时发现目光聚集到了你的发夹。我记得会有一个粉红色小立方体的发夹，一个小指甲盖圆边三角形的发夹。只是我没弄明白的是，为什么上节课戴的是这个，下节课却又换了另外一个呢?恍然间才发觉，那其实不是上节课，而是昨天了。。。而从此我也开始根据你发夹的更换与否来感知时间的流逝。</p>
<p>高中，也就这么过完了。没有其他的意外，没有奇迹的出现，只是那意料之中的结局。</p>
<p>当2018年的夏天结束，那个少年和那个女孩走出校园的红墙，就这样两人各自进入了大学。而终究长江口的风很难吹到天府之国，少年想，可能这辈子也就这样了吧。</p>
<blockquote>
<p>你说这是咎由自取，我说此乃命中注定。尽管灵魂依然纠结，我也只能选择让你离去，你那澄澈明亮的双眸，愿我拥有这道光芒，如今我已无法挣脱，只因你与我共同面对那股黑暗 《Demons》</p>
</blockquote>
<p>男孩在大学里给女孩写了一封信，那封信向女孩表达了歉意，解释到高中那段时光发生的故事最终没有一个Good Ending，只是因为没有在<strong>正确的时间</strong>，<strong>遇上正确的人</strong>啦，并且表示(狡辩)说自己过的很好。希望女孩未来也能重梳婵鬓，美扫蛾眉。女孩回信到，“那段美好的时光就让它封存在记忆里，成为最美的回忆...”。这样一来，男孩终究还是放下了那个包袱，觉得从此也不再会有遗憾了！(其实，说没遗憾是假的，只是这样会小一些吧！)</p>
<p>之后的大学生活中，男孩再次堆砌起那一层又一层的心之壁。当旁人问道男孩，为什么不喜欢社交？男孩只苦笑道：“这是自己咎由自取罢！”</p>
<h5 id="破冰">破冰！</h5>
<p>我鼓起了<strong>这辈子</strong>的一次勇气，</p>
<p>直到昨天我才知道⼤家要聚会（没有错过，正确的时间），我认为这次聚会之后可能很难再聚 ⼀起了，我很挣扎，但是⼼理兜兜转转还是决定来参加（正确的决定），如果不是那句再次⼼ 动的讯号，以及那个正确的⼈，⼜怎么会有如命运般奇迹时刻的降临呢！</p>
<p>我不是⼀个爱情命运主义者，但是我却没法找出⼀个更令⼈接受这⼀切的事实存在的理由！⾼ 中那次命运的错位导致的绳结，终于在正确的时刻，做出了正确的决定，遇上了正确的⼈。孤 独的根号三终于摆脱了那丑陋的平⽅根号。</p>
<h3 id="对未来的期望">对未来的期望</h3>
<h5 id="矛盾发生">矛盾发生</h5>
<p>话说谁没有在爱情中尝过苦头，但是当那个特殊的环境特殊的时间偶然的事件发生的时候，我们还是会一样奋不顾身的投入其中，乐此不疲，这种甜蜜的毒药，不知真的是否会有解药。</p>
<h4 id="引用">引用：</h4>
<p>爱我吧！为什么？我只有一个微不足道的理由：因为我爱你。</p>
<p>虽千万人，吾往矣</p>
<p>我写下“我爱你”，并不是这样能让你明白我对你迫切的爱，而期待你对我的回应。我这么做单纯是我脑海中成千上万上亿个神经元在我脑海中，接通了那条写下“我爱你“信息纸条的结果！</p>
<p>Love your curves and all your edges 深愛著你身上稜稜角角 All your perfect imperfections</p>
<p>我无法保证能给你童话般的世界，</p>
<p>也无法保证自己能在一夜之间长大。</p>
<p>但是我保证，</p>
<p>你可以像公主一样永远生活在自由，幸福之中。</p>
<p><img src="https://i.imgur.com/tqwYrS8.jpg" title="test" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/20/Introduction/%E6%96%87%E7%AB%A0%E4%B8%AD%E5%BF%83%EF%BC%9A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="we1k">
      <meta itemprop="description" content="mix 7 mix 8">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Notes&Words">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/20/Introduction/%E6%96%87%E7%AB%A0%E4%B8%AD%E5%BF%83%EF%BC%9A/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-20 17:41:46" itemprop="dateCreated datePublished" datetime="2021-12-20T17:41:46+08:00">2021-12-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="论文设计选题目的工作任务">论文（设计）选题目的、工作任务</h5>
<h6 id="选题目的">选题目的</h6>
<p>近年来，随着深度学习技术逐渐不断发展，我们看到了在自然语言处理问题上已经取得的巨大进展，特别是在问答系统(Question Answering)领域。但是与此同时，也有越来越多的研究证据表明在面临仅有少量训练数据样本，跨领域之间的问题时，目前的模型仍然无法在测试样本集上获得令人满意的效果。因此，如何实现一种少样本的，跨领域之间的，具有较强泛化能力的自然语言处理模型已经成为各位深度学习研究人员的重点研究对象。</p>
<p>此次论文选题的目的在于通过尝试多种的方法，探究不同模型对问答系统基准实验泛化能力的影响。此选题能够从实际的角度出发，着眼于现实世界，构造一个相对于问答系统基准实验上的，更具泛化能力的模型。</p>
<h6 id="工作任务">工作任务:</h6>
<p>A. 收集不同领域问答系统数据集</p>
<pre><code>1. 使用当前研究领域中的现有数据集，如SQuAD数据集
2. 收集本项目相关数据集</code></pre>
<p>B. 探究小样本任务中的方法 1. 元学习模型(Meta learning) 2. 数据增强(Data Augmentation) 3. 领域对抗训练模型(Domain-Adversarial)</p>
<p>C. 在强大的预训练语言模型的辅助下，构造一个问答系统</p>
<pre><code>1. 构建一个问答系统
2. 利用大规模预训练语言模型进行微调，如BERT模型，GPT2模型</code></pre>
<p>D. 分析不同方法在问答系统基准实验的影响</p>
<h5 id="目前资料收集情况含指定参考资料">目前资料收集情况（含指定参考资料）</h5>
<p>问答系统与数据集:无论是从研究角度还是从实践角度来看，问答系统都是一项重要的NLP任务。</p>
<p>斯坦福QA系统数据集: SQuAD (https://rajpurkar.github.io/SQuAD-explorer/)，</p>
<p>NewsQA: https://arxiv.org/abs/1611.09830</p>
<p>而更多数据集需要收集</p>
<p>预训练语言模型: 随着预训练数据量的增加，网络结构的逐渐复杂，模型参数的急剧上升，使得BERT，GPT等模型已经在NLP的多个任务中表现出了其自身大量预训练任务的优势。成为NLP相关任务中的炙手可热的轿子。</p>
<p>original BERT paper: https://arxiv.org/abs/1810.04805</p>
<p>RoBERTa: https://arxiv.org/pdf/1907.11692.pdf</p>
<h5 id="论文设计完成计划">论文（设计）完成计划</h5>
<ol type="a">
<li>1月上旬完成数据集收集任务</li>
<li>2月下旬完成benchmark模型构建，并尝试在benchmark上训练</li>
<li>3月上旬完成变种模型构建，并分析结果</li>
<li>4月中旬完成论文写作</li>
<li>4月下旬完成论文修改</li>
</ol>
<h4 id="开题答辩">开题答辩</h4>
<h5 id="选题意义">选题意义</h5>
<p><code>你要说清楚你的选题将对那些研究理论和那些研究项目产生怎样的推动作用，或者你的选题在实际应用中有什么指导意义。选题意义要着重突出这几点：1、你的选题是前人没有研究过的，最好有具体数据或者文章来说明这一点，这样比较有可信度。2、你的选题前人研究过，但是出于各种原因并没有研究到位，或者没有解决实际问题，或者研究的方法和结果不尽如人意。以上两点都需要你有具体的例子或者数据来证明，提高可信度</code></p>
<p>自然语言处理是实现人工智能技术中至关重要的一个环节。其赋予计算机“理解”人类语言的能力，使得计算机能够通过接受文本语言形式的输入，经过处理转换为机器语言。自然语言处理技术给计算机和人类之间搭建了一架沟通的桥梁，让人类和计算机之间的交互方式变得更加方便，更加简单。在如今网络信息爆炸增长的时代，如何高效地，精确地检索和处理信息成为了一个难题。而自然语言处理技术正是这样一个可以提供便捷地人机交互方式的重要途径。</p>
<p>近年来，随着深度学习在自然语言处理领域研究的不断深入发展，利用神经网络进行语义分析，信息检索，机器翻译等自然语言处理任务已成为主流研究方法。 相较于二十世纪提出的基于规则匹配和复杂逻辑推理的数据统计模型，深度学习的神经网络技术能够通过更好地处理语句中深层次的语义信息，实现端对端地模型训练，避免依赖统计模型中各个独立子模块的信息交互处理。</p>
<p>问答系统作为自然语言处理中的一种信息检索（Information Retrieval）技术，通过接受用户以自然语言文本（非结构化文本）形式输入问题，自动处理并返回对应问题答案的技术。作为自然语言处理的明日之星，问答系统在人们的日常生活的多个领域中拥有重大的实际应用价值。比如聊天机器人，智能客服，搜索引擎，知识图谱等。</p>
<p>在问答系统中，我们也可以根据信息来源划分为基于文本段落、基于网络文件、基于知识库等类别；根据问题类型划分为简单单领域问题和复杂多领域问题、开放性领域和限定性领域等。而在开放性领域进行自动问答则既是重点也是难点。目前提出的大多数模型仅仅考虑训练数据与测试数据都属于同一领域分布的情况，但是在现实生活中，训练数据与测试数据属于不同领域分布等情况往往更加普遍。如何利用少量的跨领域测试样本，训练出一个泛化能力强，模型收敛快，效果更好的开放性领域问答系统，是把智能问答系统应用到更加广泛的日常生活中的关键之处，也是实现强人工智能的必经之路。</p>
<p>目前还有什么问题? 比如training test dataset distribution 不一致</p>
<p>训练样本分布和测试样本分布不一致 On the other hand, humans can easily generalize beyond their training distribution–while not strictly from our training distribution, we can effortlessly understand novels set in fictional worlds and quickly understand the meanings of new words. How can we build NLP systems that generalize like humans? From a practical perspective, robustness to out-of-distribution data is critical for building accurate NLP systems in the real world since train and test data often come from distinct user interactions.</p>
<h5 id="国内外研究现状概述">国内外研究现状概述</h5>
<h5 id="问答系统-信息检索">问答系统， 信息检索</h5>
<p>当问答系统接受到用户的自然语言形式输入后，经过自然语言理解，获取语句深层次语义，然后在信息资源库中进行信息检索，最终通过答案抽取或者答案生成输出问题对应答案。</p>
<p>QA问答系统</p>
<p>小样本学习</p>
<p>BERT模型</p>
<p>数据增强</p>
<h5 id="主要研究内容">主要研究内容</h5>
<p><img src="C:\Users\we1k\AppData\Roaming\Typora\typora-user-images\image-20211220165556840.png" alt="image-20211220165556840" style="zoom:67%;" /></p>
<p>(Bidirectional Encoder Representation from Transformers)</p>
<p>当问答系统接受到用户的自然语言形式输入后，经过自然语言理解（Natural Language Understanding），获取语句深层次语义，然后在信息资源库中进行信息检索，</p>
<h5 id="section"></h5>
<p>QA问答系统</p>
<p>For QA tasks, models must read passages of text and answer questions about them. Specifically, given a context passage of text and a question to answer, QA models must choose a span of contiguous words within the passage that contains the answer to the question posed. This problem is challenging because models must accurately learn the meaning of both the context passage and the corresponding question, as well as the relationship between them.</p>
<p>小样本学习</p>
<p>BERT模型</p>
<p>数据增强</p>
<h5 id="拟采用的研究思路">拟采用的研究思路</h5>
<h5 id="研究工作安排和进度">研究工作安排和进度</h5>
<h5 id="参考文献">参考文献</h5>
<p>https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.5090150306</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/18/%E6%9D%82%E8%B0%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="we1k">
      <meta itemprop="description" content="mix 7 mix 8">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Notes&Words">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/18/%E6%9D%82%E8%B0%88/" class="post-title-link" itemprop="url">杂谈</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-12-18 09:39:48 / Modified: 09:41:21" itemprop="dateCreated datePublished" datetime="2021-12-18T09:39:48+08:00">2021-12-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="杂谈">杂谈</h2>
<p>在人类文明的漫漫长河中，有一个问题始终困扰着我们，即人的一生终究在追求着什么？这像一场需要用尽一生去作答的考试，但试卷上却是一片空白。有的人在考试中手不停挥，抓起手中的彩笔，希望在考试结束前画出最浓墨重彩的自己；有的人沉思良久，心中却仍彷徨四顾，在试卷上几经修改后，才发现所剩留白已然不多；有的人对考试置若罔闻，只在最显眼处草草几笔后便提前交卷。诚然每个人都在答卷上书写着自己的人生—那是属于他们独一无二的人生标记。如果将古往今来所有人的试卷汇集到一起，便编成一部熠熠生辉的人类史。</p>
<p>我一直都在尝试着去更多的了解这个世界。希望通过观察世界的轨迹，探究到一套理论来对世界进行客观的诠释。于是我常记录下一些平凡的瞬间，留下自己思考的印记，在相片上，也在脑海里。这是我在以我自己的方式记录世界运行的方式。</p>
<p>时间在不断向前，却把记忆中的平凡瞬间冲刷得弥足珍贵。荣耀，喜悦，亦或是使我更加坚定的伤痛；感动，思念，或是藏在心底的秘密。过去的片段组成了现在的我，积累成了未来的我。I am because I was.</p>
<p>但历史上却不乏诸多悲观主义者，他们大多在略尽沧桑后，留下“人生不值得“的讥讽箴言。人生一载，本多艰难。我们固然无法长久的把握幸福，容易陷入情欲的围困，甩不掉对地位的痴迷，在意外面前不堪一击，并都毫无例外地，在寸寸折磨中走向死亡。</p>
<p>“我在水面上行走，但我却不是耶稣“。可我们还是会继续活着啊！</p>
<p>尼采说他自己是太阳，但他终究不是太阳。最终燃尽自己也没能照亮整个世界，但我也不是尼采，终不能像他一样痴狂，过度的唯心主义只能使我愈加不安。而竹林七贤放浪形骸，倜傥不羁，最后却也碌碌无为。但我也不是他们，过分的消极态度只能使我虚度人生。</p>
<p>思考太多也不一定是一件好事，那么何须顾及太多呢？往者不可谏，来者犹可追。正如安东尼在《云治》里说所，就算世界上的一切都是假的，但是这并不妨碍我此时此刻正爱着这个世界啊。</p>
<p>Seize the day</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/14/Meta-learning-MAML-implement/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="we1k">
      <meta itemprop="description" content="mix 7 mix 8">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Notes&Words">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/14/Meta-learning-MAML-implement/" class="post-title-link" itemprop="url">Meta learning -- MAML reimplement</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-14 16:44:38" itemprop="dateCreated datePublished" datetime="2021-12-14T16:44:38+08:00">2021-12-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-12 14:21:40" itemprop="dateModified" datetime="2022-02-12T14:21:40+08:00">2022-02-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="intro">Intro:</h3>
<p>想着毕设中要用到meta learning的方法，于是打算提前复现一遍。再次之前对meta learning的认知只是停留在浅尝辄止的程度，今日得以深入了解，一探MAML真容。</p>
<h3 id="context">Context:</h3>
<p>Meta learning究其根源：learning2learn，即是想要学习一个函数F，对子任务f的学习进行优化。</p>
<h5 id="首先需要知道的是meta-learning与model-pretraining的区别">首先需要知道的是meta learning与model pretraining的区别:</h5>
<p>对meta learning来说，需要学习一个参数<span class="math inline">\(\Phi\)</span>，使得此参数在多个样本集上<strong>再次训练</strong>后，更快，更好地收敛。 简单来说就是<strong>当前潜力好</strong>。 <span class="math display">\[
L(\Phi) = \sum^{N}_{n=1}l^n(\theta^n)
\]</span> 对model pretraining来说，需要学习一个参数<span class="math inline">\(\Phi\)</span>，使得此参数在多个样本集上<strong>此时</strong>已经获得最小loss。简单来说就是<strong>当前效果好</strong>。 <span class="math display">\[
L(\Phi) = \sum^{N}_{n=1}l^n(\Phi)
\]</span> <em>注意此时model pretrain对于多个模型来说都是使用的同一个参数</em></p>
<p><img src="image-20211214171911491.png" alt="Source of images: http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf" style="zoom:67%;" /></p>
<p><img src="image-20211214171925530.png" alt="Source of images: http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf" style="zoom:67%;" /></p>
<p>对于MAML中，我们只考虑如何对meta模型的初始化参数<span class="math inline">\(\Phi\)</span>进行学习。怎样对<span class="math inline">\(F(\Phi)\)</span>进行学习，使得针对一个新的任务能更快收敛呢? 我们首先做出如下定义：</p>
<h5 id="notions">Notions:</h5>
<ol type="1">
<li><p><strong>N-way K-shot classification</strong>: In each training and test tasks, there are N classes, each has K examples.</p></li>
<li><p>meta model : 学习如何初始化<span class="math inline">\(\Phi\)</span>，使得在子任务中更快收敛的模型。设定其lr=<span class="math inline">\(\mu\)</span> 。</p>
<p><span class="math display">\[
loss\_function:L(\Phi)=\sum l(\theta ^n)\\
back\_propagation:\Phi = \Phi\ -\ \mu \nabla_{\Phi}L(\Phi)\\ {\tag1}
\]</span></p></li>
<li><p>sub_model: 学习N-way K-shot子任务的模型，模型参数 <span class="math inline">\(\theta\)</span> 由 <span class="math inline">\(\Phi\)</span> 初始化(clone)得到。设定其lr=<span class="math inline">\(\epsilon\)</span>，具有loss function: <span class="math inline">\(l(\theta)\)</span>。注意这里的<span class="math inline">\(l(\theta)\)</span>是根据N-way K-shot的分类子任务的loss。 <span class="math display">\[
loss\_function:l_{submodel}(\theta ^n)=l_n(\theta )\\
back\_propagation:\theta_{i+1} = \theta_i  -\epsilon \nabla_{\theta}l_n(\theta) {\tag2}
\]</span></p></li>
<li><p><span class="math inline">\(\Phi\)</span>: meta model initialization parameters</p></li>
<li><p><span class="math inline">\(\theta ^n\)</span>: model learned from task n, which depends on <span class="math inline">\(\Phi\)</span> <span class="math display">\[
\theta_0 = \Phi.clone()
\]</span></p></li>
<li><p><span class="math inline">\(l(\theta^n)\)</span> : loss of task n on the testing set</p></li>
<li><p>L(<span class="math inline">\(\Phi\)</span>) ：loss of meta learning model</p></li>
</ol>
<p>For simplity, we only consider one-step updating on submodel. As <span class="math inline">\(\theta\)</span> was copied from meta model initialization parameter <span class="math inline">\(\Phi\)</span> , which <span class="math inline">\(\theta\)</span> is identical to <span class="math inline">\(\Phi\)</span>, therefore, updating of <span class="math inline">\(\theta\)</span> substitution : <span class="math display">\[
\theta = \Phi - lr_{submodel} * \nabla_{\Phi}l_n(\Phi){\tag3}
\]</span></p>
<h3 id="methodology">Methodology:</h3>
<h4 id="step-1.">STEP 1.</h4>
<p>Initializating submodel by cloning from meta model. <span class="math display">\[
submodel = meta\_model.clone()\\
\theta_0=\Phi
\]</span></p>
<h4 id="step-2.">STEP 2.</h4>
<p>Calculating forward compution for submodel. x, y are training samples and labels of subtask. <span class="math display">\[
l_n(\theta_0) = l_{submodel}(x, y |\theta_0)
\]</span></p>
<h4 id="step-3.">STEP 3.</h4>
<p>Calculating baward compution of submodel, and update its parameters via equation 3. <span class="math display">\[
\theta_1 = \Phi - lr_{submodel} * \nabla_{\Phi}l_n(\Phi){\tag3}
\]</span></p>
<h4 id="step-4.">STEP 4.</h4>
<p>Updating Meta model. <span class="math display">\[
\Phi=\Phi-\mu\nabla_{\Phi}L(\Phi)\\
=\Phi-\mu\nabla_{\Phi}\sum l_i(\theta^n)\\
\]</span> According to first-order approximation <code>further details in appendix 1</code> : <span class="math inline">\(\nabla_{\Phi} l_n(\theta)=\nabla_{\theta}l_n(\theta)\)</span></p>
<p>Note that, we have already updating <span class="math inline">\(\theta\)</span> in STEP 3, <span class="math inline">\(\nabla\theta l(\theta^n)\)</span> should be calculated by training on another batch of the submodel task, then backward!!!!!</p>
<p>Therefore, we update meta model by using newly updating submodel gradient on a new batch of submodel task on parameter <span class="math inline">\(\theta_1\)</span>.</p>
<p>following equation: <span class="math display">\[
\Phi =\Phi-\mu\nabla_{\theta_1} \sum l_i(\theta_1){\tag 4}
\]</span></p>
<h5 id="workflow-showed-below">Workflow showed below:</h5>
<p>Slide copied from : http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf</p>
<p><img src="2.png" /></p>
<h3 id="implement-details">Implement details:</h3>
<ol type="1">
<li>子任务模型由meta任务模型初始化得到（即是由 <span class="math inline">\(\Phi\)</span> 初始化 <span class="math inline">\(\theta^n\)</span> 的过程），参数由meta model参数clone得到。此时sub weight应该是torch.Tensor，不能使用torch.optim更新，需要自己实现gradient update。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MetaLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="comment"># 继承上次module的参数</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>():</span></span><br><span class="line">		sub_model.weight = meta_model.weight.clone()</span><br><span class="line">	...</span><br><span class="line">	</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">init_weight</span>)</span></span><br><span class="line"><span class="function">  <span class="title">def</span> <span class="title">update</span>(<span class="params">meta_model</span>):</span></span><br><span class="line">      <span class="comment"># 这里的Meta_model是identical to sub_model</span></span><br><span class="line">      <span class="comment"># 更新sub_model时，由于sub_model clone自meta model, 所以sub_model.grad=None. </span></span><br><span class="line">      <span class="comment"># 我们需要使用 sub_model.weight = sub_model.weight - lr * meta_model.weight.grad</span></span><br><span class="line">      <span class="comment"># 即时上文中</span></span><br><span class="line">		 <span class="keyword">for</span> layer, parent_layer <span class="keyword">in</span> <span class="built_in">zip</span>(self.model, meta_model):</span><br><span class="line">          layer.weight -= lr * meta_layer.grad</span><br><span class="line">          layer.bias -= lr * meta_layer.bias</span><br><span class="line">      ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Meta_model</span>():</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>():</span></span><br><span class="line">      <span class="comment"># 这里的Meta_model是identical to sub_model</span></span><br><span class="line">      self.model = Model()</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_submodel</span>(<span class="params">N</span>):</span></span><br><span class="line">      sub_model = [Model(self.model) <span class="keyword">for</span> N times] </span><br></pre></td></tr></table></figure></li>
<li><h4 id="对maml进行训练这里是最关键的地方-根据methodology写出相应的step">对MAML进行训练(这里是最关键的地方!) 根据methodology写出相应的step</h4></li>
</ol>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">N-way K-shot training</span></span><br><span class="line"><span class="string">param: N: number of different support set, which we need to train N submodel simultaneously -- N way</span></span><br><span class="line"><span class="string">param: K: number of each support set&#x27;s samples -- K shot</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 模型初始化</span></span><br><span class="line">pretrain_model = Model()</span><br><span class="line">meta_model = Meta_model()</span><br><span class="line"></span><br><span class="line">STEP <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">sub_model = meta_model.get_submodel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="comment"># x.shape -&gt; (N, K, 1)</span></span><br><span class="line">	<span class="comment"># split x into (N, :n_sample, 1) as frist batch, </span></span><br><span class="line">    <span class="comment"># rest samples (N, :n_sample, 1) as second batch </span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="built_in">range</span>(N):</span><br><span class="line">    <span class="comment"># Training Pretrain_model </span></span><br><span class="line">    	p_output = pretrain_model(x[i][:n_sample, :])</span><br><span class="line">        p_loss = loss_fn()</span><br><span class="line">        p_loss.backward()</span><br><span class="line">        p_optimizer.step()</span><br><span class="line">    <span class="comment">#  这里有点类似于把K-shot切分为两个batch</span></span><br><span class="line">        p_optimizer.zero_grad()</span><br><span class="line">        output = pretrain_model(x[num_model][n_sample:, :])</span><br><span class="line">        p_loss = loss_fn()</span><br><span class="line">        p_loss.backward()</span><br><span class="line">        p_optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Training Meta_model</span></span><br><span class="line">    STEP <span class="number">2.</span></span><br><span class="line">    	meta_model.zero_grad()</span><br><span class="line">        output = sub_model[num_model](x[num_model][:n_sample, :])</span><br><span class="line">    STEP <span class="number">3.</span></span><br><span class="line">        sub_loss = loss_fn()</span><br><span class="line">        <span class="comment"># why create_graph=True ?</span></span><br><span class="line">        sub_loss.backward(create_graph=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#  sub_model.grad=meta_model.grad due to clone function, 需要使用update更新</span></span><br><span class="line">        sub_model[num_model].update(meta_model, lr)</span><br><span class="line">	STEP <span class="number">4.</span></span><br><span class="line">        meta_optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 这时的submodel的theta已经进行过更新，在new batch上计算meta_model.grad梯度更新</span></span><br><span class="line">        output = sub_model[num_model](x[num_model][n_sample:, :])</span><br><span class="line">    	<span class="comment"># 这里的sub_new_loss是为了算grad&#123;\theta&#125; l(\theta^n)</span></span><br><span class="line">        sub_new_loss = loss_fn(output, y[num_model][:n_sample, :])</span><br><span class="line">        meta_loss += sub_new_loss</span><br></pre></td></tr></table></figure></p>
<h4 id="qas">QAs:</h4>
<h5 id="为什么这里sub_loss.backwardcreate_graphtrue">1.为什么这里sub_loss.backward(create_graph=True)?</h5>
<p>这个问题时我在reimplement中遇到最大的困难。在网上查阅资料时，有人指出是因为<code>計算第一次 gradient 並保留計算圖以接著計算更高階的 gradient</code>。我并没有理解为什么需要使用更高阶的grad。</p>
<p>最终在知乎上查找到一个比较满意的答案来作为toy example。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br><span class="line"></span><br><span class="line">a = t.Tensor([<span class="number">5</span>])</span><br><span class="line">a.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">b = t.Tensor([<span class="number">6</span>])</span><br><span class="line">b.requires_grad = <span class="literal">True</span></span><br><span class="line">c = b**<span class="number">3</span></span><br><span class="line"></span><br><span class="line">z = a+c+<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一阶导数，a,b,c 都有</span></span><br><span class="line">t.autograd.grad(z, [a,b,c], create_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># t.autograd.grad(z, [a,b,c], create_graph=False) </span></span><br><span class="line">ga,gb,gc = t.autograd.grad(z, [a,b,c], create_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二阶导数</span></span><br><span class="line">t.autograd.grad(gb[<span class="number">0</span>],b,create_graph=<span class="literal">True</span>)</span><br><span class="line">t.autograd.grad(gb,b,grad_outputs=t.Tensor([<span class="number">1.</span>]),create_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三阶导数</span></span><br><span class="line">ggb = t.autograd.grad(gb[<span class="number">0</span>],b,create_graph=<span class="literal">True</span>)</span><br><span class="line">t.autograd.grad(ggb[<span class="number">0</span>],b,create_graph=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>其中ga只能计算出一阶导数，gga二阶导数为0，而对于gb，ggb可以计算出二阶导数。要想保留gb对于b的偏导，我们需要再次加入create_graph=True.</p>
<p><code>梦里寻他千百度，蓦然回首，那人却在灯火阑珊处</code></p>
<p>Step3中我们通过first batch训练的loss反向传播到meta_model.grad更新到submodel的<span class="math inline">\(\theta_1\)</span>. 注意这时<span class="math inline">\(\Phi\rightarrow\theta_0\)</span>是依次传导梯度的。这时我们需要由<span class="math inline">\(\theta_0\rightarrow\theta_1\)</span>进行step3的更新。如果设置backward()中create_graph=false。则<span class="math inline">\(\theta_1\)</span>是不在具有对<span class="math inline">\(\Phi\)</span>的梯度的，那么在second batch进行更新的时候，再次把second batch loss反向传导便无法达到meta model,自然也不能让meta model参数<span class="math inline">\(\Phi\)</span>与<span class="math inline">\(\theta_1\)</span>具有同样的梯度了。</p>
<p>In short, 设置create graph就是为了在第二次更新<span class="math inline">\(\theta_1\)</span>时，让<span class="math inline">\(\Phi\)</span>也朝向相同grad方向下降。(即使meta model和submodel学习率可能不同)</p>
<h5 id="为什么要使用metalayer">2.为什么要使用MetaLayer?</h5>
<p>解决上个问题后，这个问题也就迎刃而解了。MetaLayer设置的目的是为了让meta model中的梯度和submodel的梯度一致。反过来，若直接构造个和Meta model模型一样的模型，那么新模型中的参数Parameter的属性会导致新模型拥有自己独一的梯度。</p>
<p>根据first order approximation, submodel只是一个用于替meta model计算梯度的工具，用于近似meta model计算其梯度更新 <span class="math display">\[
\nabla_{\Phi} l_n(\theta)=\nabla_{\theta}l_n(\theta)
\]</span></p>
<h3 id="reference">Reference:</h3>
<p>[1]. http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf</p>
<p>[2].MAML : Chelsea Finn, Pieter Abbeel, and Sergey Levine, “ModelAgnostic Meta-Learning for Fast Adaptation of Deep Networks”, ICML, 2017</p>
<p>[3]. https://zhuanlan.zhihu.com/p/151384364</p>
<h3 id="appendix">Appendix:</h3>
<h4 id="first-order-approximation">1.First order approximation</h4>
<p><img src="image-20211214172315594-16395326567571.png" alt="image-20211214172315594"  /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/13/Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="we1k">
      <meta itemprop="description" content="mix 7 mix 8">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Notes&Words">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/13/Introduction/" class="post-title-link" itemprop="url">FQA</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-13 10:12:31" itemprop="dateCreated datePublished" datetime="2021-12-13T10:12:31+08:00">2021-12-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-17 14:56:26" itemprop="dateModified" datetime="2021-12-17T14:56:26+08:00">2021-12-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="q-为什么引入非线性激励函数">Q: 为什么引入非线性激励函数？</h5>
<p>A:如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了</p>
<h5 id="qwhy-relu-max0x">Q:Why Relu = max(0,x)</h5>
<p>A:1.仿生物学原理：在一个使用修正线性单元（即线性整流）的神经网络中大概有50%的神经元处于激活态。</p>
<p>2.更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题</p>
<p>3.简化计算过程：没有了其他复杂激活函数中诸如指数函数的影响；</p>
<h5 id="q-loss-函数中-mse-和-categorical_crossentropy交叉熵损失函数区别">Q: loss 函数中 mse 和 categorical_crossentropy（交叉熵损失函数）区别</h5>
<p>交叉熵：实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。0为最小值。</p>
<p>mse 在梯度下降时<img src="1.jpgs" alt="bp" /></p>
<p>sigmoid函数导数会在z取大部分值时会很小，这样会使得w和b更新非常慢</p>
<h5 id="动量momentum优化-学习率衰减每次参数更新后">动量（momentum）优化？ 学习率衰减（每次参数更新后）？</h5>
<h4 id="leakyrelu">LeakyRelu：</h4>
<p><strong>ReLU</strong> ReLU函数代表的的是“修正线性单元”，它是带有卷积图像的输入x的最大函数(x,o)。ReLU函数将矩阵x内所有负值都设为零，其余的值不变。ReLU函数的计算是在卷积之后进行的，因此它与tanh函数和sigmoid函数一样，同属于“非线性激活函数”。这一内容是由Geoff Hinton首次提出的。 <strong>ELUs</strong> ELUs是“指数线性单元”，它试图将激活函数的平均值接近零，从而加快学习的速度。同时，它还能通过正值的标识来避免梯度消失的问题。根据一些研究，ELUs分类精确度是高于ReLUs的。下面是关于ELU细节信息的详细介绍：</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0701/A9B535C61C2D63E152DE2CEECB4531EE83E80208_size26_w740_h230.jpeg" /></p>
<p><strong>Leaky ReLUs tanh</strong> ReLU是将所有的负值都设为零，相反，Leaky ReLU是给所有负值赋予一个非零斜率。Leaky ReLU激活函数是在声学模型（2013）中首次提出的。以数学的方式我们可以表示为：</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0701/CFC5A1C95A84A6D8CF3FFC1DD30597782AEEAE57_size20_w740_h231.jpeg" />ai是（1，+∞）区间内的固定参数。</p>
<p><strong>参数化修正线性单元（PReLU）</strong> PReLU可以看作是Leaky ReLU的一个变体。在PReLU中，负值部分的斜率是根据数据来定的，而非预先定义的。作者称，在ImageNet分类（2015，Russakovsky等）上，PReLU是超越人类分类水平的关键所在。 <strong>随机纠正线性单元（RReLU）</strong> “随机纠正线性单元”RReLU也是Leaky ReLU的一个变体。在RReLU中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU的亮点在于，在训练环节中，aji是从一个均匀的分布U(I,u)中随机抽取的数值。形式上来说，我们能得到以下结果：<em> </em></p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0701/B3F2F3EA627EBB55D88C8F8FB36942C56B350A4B_size14_w740_h221.jpeg" /></p>
<p><strong>总结</strong> 下图是ReLU、Leaky ReLU、PReLU和RReLU的比较：</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0701/C56E5C6FCBB36E70BA5EBC90CBD142BA320B3DF6_size19_w740_h217.jpeg" /></p>
<h4 id="rmsprop-and-adam-optimizer">RMSprop and Adam optimizer</h4>
<p><span class="math display">\[
\begin{equation}
SGD: x = x-\alpha*dx\\
\end{equation}
\]</span></p>
<p><span class="math display">\[\begin{equation}
momentum: v=\beta*v-(1-dampening)*dx\\
x=x+\alpha v
\end{equation}\]</span></p>
<p>default dampening=0</p>
<p>RMSprop：使用二阶矩估计。</p>
<p><span class="math inline">\(x=x-\alpha*\frac{E(dx)}{\sqrt{E(d^2x)}}\)</span></p>
<p>结合梯度平方的指数移动平均数来调节学习率的变化，使得函数在Non-stationary的目标函数情况下进行很好地收敛。 <span class="math display">\[
v_t=\beta(v_{t-1})+(1-\beta)*d^2x\\
x=x-\alpha*\frac{dx}{\sqrt{v_t}+\theta}\\
x=x-\alpha*\frac{dx}{\sqrt{E(d^2x)}}
\]</span> 其中<span class="math inline">\(\gamma\)</span>一般取值为0.9左右，而<span class="math inline">\(\theta\)</span>只是为了让除数非0，所以一般取值为1e-8。</p>
<p>Adam:结合一阶矩估计和二阶矩估计 <span class="math display">\[
HINT:x=x-\alpha*\frac{E(dx)}{\sqrt{E(d^2x)}}\\
m_t=\beta_1*m_{t-1}+(1-\beta_1)*dx\\
v_t=\beta_2*v_{t-1}+(1-\beta_2)*d^2x\\
x=x-\alpha*\frac{m_t}{np.sqrt(v)+\theta}\\
\]</span> Further:修正m_t和v_t的bias：</p>
<p><img src="1303172-20180103142455081-1181552338.png" /></p>
<p><img src="1303172-20180103142456049-225137169.png" /></p>
<p><img src="1303172-20180103142457034-700230840.png" /></p>
<h4 id="自定义layers">自定义layers</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensorflow version</span></span><br><span class="line"><span class="meta">@tf.custom_gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GradientReversalOperator</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grad</span>(<span class="params">dy</span>):</span></span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span> * dy</span><br><span class="line">    <span class="keyword">return</span> x, grad</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientReversalLayer</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GradientReversalLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> GradientReversalOperator(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch version</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear_fn</span>(<span class="params">torch.autograd.Function</span>):</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, weight, bias=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.save_for_backward(<span class="built_in">input</span>, weight, bias)</span><br><span class="line">        <span class="comment"># equal to</span></span><br><span class="line">        <span class="comment"># self.inputs = inputs</span></span><br><span class="line">        <span class="comment"># self.weights = weights</span></span><br><span class="line">        <span class="comment"># self.bias = bias</span></span><br><span class="line">​        output = <span class="built_in">input</span> @ weight.t()</span><br><span class="line">​        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">​            output += bias</span><br><span class="line">​        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, grad_output</span>):</span></span><br><span class="line">        <span class="built_in">input</span>, weight, bias = self.saved_tensors</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># correslating to upper alternative        </span></span><br><span class="line">        <span class="comment"># input, weight, bias = self.input, self.weight, self.bias</span></span><br><span class="line">        <span class="comment"># self.bias -&gt;scalar</span></span><br><span class="line">        <span class="keyword">return</span> grad_output @ weight, grad_output.t() @ <span class="built_in">input</span>, grad_output.<span class="built_in">sum</span>(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="upsampling2d---7-7-32---14-14-32-改变特征层的长和宽">Upsampling2D() -&gt; (7, 7 , 32) - &gt; (14 ,14 ,32） 改变特征层的长和宽</h4>
<h4 id="q先bn还是先激活">Q:先BN还是先激活？</h4>
<p>A: 先BN在激活。https://www.zhihu.com/question/318354788</p>
<p>Batch Norm方法经过规范化和缩放平移，可以使输入数据，重新回到非饱和区，还可以更进一步：控制激活的饱和程度，或是非饱和函数抑制与激活的范围。</p>
<p>从剃度消失的角度来看，比如sigmoid激活函数，两边的剃度很小，容易剃度消失，bn的作用是把输出拉回到非饱和区，就是剃度大的那部分，所以要先bn再激活。</p>
<h5 id="q-tf-中的padding-参数same-vaild">Q: tf 中的padding 参数same, vaild</h5>
<figure>
<img src="image-20200905231349413.png" alt="padding=&#39;same&#39; or &#39;vaild&#39;" /><figcaption aria-hidden="true">padding='same' or 'vaild'</figcaption>
</figure>
<p>outputs_size = (inputs + 2*padding - kernel_size ) // strides + 1</p>
<h4 id="batchnormalization-layernormalization作用">BatchNormalization, LayerNormalization作用:</h4>
<ul>
<li><strong>BatchNorm</strong>：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布。(<strong><em><u>B</u></em></strong>, C, H, W)，注意这里的<span class="math inline">\(\gamma和\beta\)</span>都是learnable Parameters shape of <strong>(C, 1)</strong>。所以根据公式可以得出<span class="math inline">\(\gamma\)</span>可以学到各个channel之间的重要性。
<ul>
<li><img src="image-20211217142710550.png" title="fig:" alt="image-20211217142710550" /></li>
</ul></li>
<li><strong>LayerNorm</strong>：channel方向做归一化，算CHW的均值，主要对RNN作用明显。公式如上图。
<ul>
<li>(B, <strong><u><em>C</em></u></strong>, H, W)，注意这里的<span class="math inline">\(\gamma和\beta\)</span>都是learnable Parameters shape of <strong>(B, 1)</strong>。所以根据公式可以得出<span class="math inline">\(\gamma\)</span>可以学到各个<strong>batch</strong>之间的重要性。</li>
</ul></li>
</ul>
<figure>
<img src="https://img-blog.csdnimg.cn/20181230223445264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYW5nbGlhbmxt,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<p>标准正态分布更容易让梯度下降速度提高。</p>
<p>右图为标准化结果</p>
<figure>
<img src="image-20200920092801730.png" alt="Batch Normalization" /><figcaption aria-hidden="true">Batch Normalization</figcaption>
</figure>
<h4 id="auc评判标准roc曲线下的面积01">AUC评判标准：ROC曲线下的面积【0,1】</h4>
<h4 id="l1l2正则化l2正则化">L1,L2正则化L2正则化</h4>
<p>在深度学习中，用的比较多的正则化技术是L2正则化，其形式是在原先的损失函数后边再加多一项：<span class="math inline">\(1/2λ\Sigmaθ^2_i\)</span>，那加上L2正则项的损失函数就可以表示为：L(θ)=L(θ)+λ∑niθ2iL(θ)=L(θ)+λ∑inθi2，其中θθ就是网络层的待学习的参数，λλ则控制正则项的大小，较大的取值将较大程度约束模型复杂度，反之亦然。</p>
<p>L2约束通常对稀疏的有尖峰的权重向量施加大的惩罚，而偏好于均匀的参数。这样的效果是鼓励神经单元利用上层的所有输入，而不是部分输入。所以L2正则项加入之后，权重的绝对值大小就会整体倾向于减少，尤其不会出现特别大的值（比如噪声），即网络偏向于学习比较小的权重。所以L2正则化在深度学习中还有个名字叫做“权重衰减”（weight decay），也有一种理解这种衰减是对权值的一种惩罚，所以有些书里把L2正则化的这一项叫做惩罚项（penalty）。</p>
<p>我们通过一个例子形象理解一下L2正则化的作用，考虑一个只有两个参数w1w1和w2w2的模型，其损失函数曲面如下图所示。从a可以看出，最小值所在是一条线，整个曲面看起来就像是一个山脊。那么这样的山脊曲面就会对应无数个参数组合，单纯使用梯度下降法难以得到确定解。但是这样的目标函数若加上一项0.1×(w21+w22)0.1×(w1<sup>2+w2</sup>2)，则曲面就会变成b图的曲面，最小值所在的位置就会从一条山岭变成一个山谷了,此时我们搜索该目标函数的最小值就比先前容易了，所以L2正则化在机器学习中也叫做“岭回归”（ridge regression）。</p>
<p><img src="正则规范化对曲面影响.png" /></p>
<h3 id="attention">Attention：</h3>
<h5 id="why-not-seq2seq-model">Why not seq2seq model？</h5>
<p>the encoder and decoder of a seq2seq model.</p>
<ul>
<li><p>The encoder process the input sequence and compresses the information into a <strong>context vector</strong>. BUT The context vector is a fixed length vector. This process is referred as <strong>Embedding</strong>. which is expected to get a good summary of the whole input senquence.</p></li>
<li><p>A <strong>decoder</strong> is initialized with the context vector to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state.</p>
<figure>
<img src="https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png" alt="encoder-decoder model with additive attention layer" /><figcaption aria-hidden="true">encoder-decoder model with additive attention layer</figcaption>
</figure></li>
</ul>
<p>The secret sauce of attention is to <strong>create a shortcuts between the context vector and the entire input sequence</strong>.</p>
<p>Essentially the context vector consumes three pieces of information:</p>
<ul>
<li>encoder hidden states;</li>
<li>decoder hidden states;</li>
<li>alignment between source and target.</li>
</ul>
<figure>
<img src="https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-attention.png" alt="encoder-decoder model with additive attention layer" /><figcaption aria-hidden="true">encoder-decoder model with additive attention layer</figcaption>
</figure>
<h4 id="skip-gram">Skip-gram</h4>
<p>.More formally, given a sequence of training words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average log probability</p>
<figure>
<img src="image-20201109143518576.png" alt="LM model objective" /><figcaption aria-hidden="true">LM model objective</figcaption>
</figure>
<p>where c is the size of the training context (which can be a function of the center word wt). Larger c results in more training examples and thus can lead to a higher accuracy, at the expense of the 2 training time. The basic Skip-gram formulation defines p(wt+j |wt) using the softmax function:</p>
<figure>
<img src="image-20201109143602058.png" alt="Skip-Gram" /><figcaption aria-hidden="true">Skip-Gram</figcaption>
</figure>
<p>where <span class="math inline">\(v_w\)</span> and <span class="math inline">\(v ′_ w\)</span> are the “input” and “output” vector representations of w, and W is the number of words in the vocabulary. This formulation is impractical because the cost of computing gradient p(wo|wi) is proportional to W.</p>
<h4 id="negative-sampling">Negative Sampling</h4>
<p>While NCE can be shown to approximately maximize the log probability of the softmax, the Skip-gram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality. We define Negative sampling (NEG) by the objective</p>
<p>MAX : <span class="math inline">\(log σ(v ′ {wO}^ ⊤ v_{wI} ) +\sum^ k _{i=1} Ewi∼Pn(w) [log σ(−v ′ _{wi}^ ⊤ v_{wI} ) ]\)</span></p>
<p>which is used to replace every log P(wO|wI ) term in the Skip-gram objective. Thus the task is to distinguish the target word wO from draws from the noise distribution Pn(w) using logistic regression, where there are k negative samples for each data sample.</p>
<p><strong>判别式模型</strong>（Discriminative Model）是直接对条件概率p(y|x;θ)建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。</p>
<p><strong>生成式模型</strong>（Generative Model）则会对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi，即：CRF, naive Bayesian</p>
<h4 id="torch中tensor-parameter-variable的区别">torch中Tensor, Parameter, Variable的区别</h4>
<p>从0.4起， Variable 正式合并入Tensor类，通过Variable嵌套实现的自动微分功能已经整合进入了Tensor类中。虽然为了代码的兼容性还是可以使用Variable(tensor)这种方式进行嵌套，但是这个操作其实什么都没做。</p>
<p>所以，以后的代码建议直接使用Tensor类进行操作，因为官方文档中已经将Variable设置成过期模块。</p>
<p>要想通过Tensor类本身就支持了使用autograd功能，只需要设置.requires_grad=True</p>
<p><code>x = torch.rand(5, 5, requires_grad=True)</code></p>
<p><code>Tensor</code>: torch中操作的基本单位，等价于numpy中的向量array，能够与array相互转换. torch.from_numpy(), .numpy(), Variable类中的的grad和grad_fn属性已经整合进入了Tensor类中</p>
<p><code>Parameter</code>: 属于Tensor的子类。通过model.parameters()获取对模型需要更新的参数，并通过optimizer(model.parameters(), lr)来反向传播</p>
<h6 id="tensor.requires_grad属性跟随">tensor.requires_grad属性跟随</h6>
<p>在张量间的计算过程中，如果在所有输入中，有一个输入需要求导，那么输出一定会需要求导；相反，只有当所有输入都不需要求导的时候，输出才会不需要。</p>
<p>在训练过程中，虽然input默认requires_grad=False，但是model(nn.Module)中使用的model.parameters()是继承与Parameter，因此output以及loss_digits会默认带有grad</p>
<p>若需要获取model中所有层的参数，则需要使用model.named_parameters()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters(): <span class="comment"># OrderDict type</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters(): </span><br><span class="line">    <span class="comment"># 这里的param是nn.Parameter()</span></span><br><span class="line">    <span class="comment"># 可以通过param.grad获取grad</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> model.state_dict(): <span class="comment">#generator type</span></span><br><span class="line">	<span class="built_in">print</span>(model.state_dict()[name])</span><br></pre></td></tr></table></figure>
<h4 id="depthwise-pointwise-separable-convolution">DepthWise, PointWise, Separable Convolution</h4>
<p>Images from: https://yinguobing.com/separable-convolution/</p>
<h5 id="standard-conv-nn.conv2din_ch-out_ch-param-in_ch-out_ch-kernel-kernel">Standard conv: nn.Conv2d(in_ch, out_ch), param: in_ch * out_ch * kernel * kernel</h5>
<figure>
<img src="https://yinguobing.com/content/images/2018/02/conv-std.jpg" alt="conv-std" /><figcaption aria-hidden="true">conv-std</figcaption>
</figure>
<h5 id="depthwise-conv-nn.conv2din_ch-kin_ch-3-1-1-groupsin_ch-param-k-in_ch-kenrel-kenrel">Depthwise conv: nn.Conv2d(in_ch, k<em>in_ch, 3, 1, 1, groups=in_ch), param: (k </em> in_ch) * kenrel * kenrel</h5>
<figure>
<img src="https://yinguobing.com/content/images/2018/02/depthwise-conv.jpg" alt="depthwise-conv" /><figcaption aria-hidden="true">depthwise-conv</figcaption>
</figure>
<h5 id="pointwise-conv-nn.conv2din_ch-out_ch-1-param-in_ch-out_ch-1-1">Pointwise conv : nn.Conv2d(in_ch, out_ch, 1), param : in_ch * out_ch * 1 * 1</h5>
<p>注意，经验性的一般Pointwise Conv后不接激活层，接激活后效果变差</p>
<figure>
<img src="https://yinguobing.com/content/images/2018/02/pointwise-conv.jpg" alt="pointwise-conv" /><figcaption aria-hidden="true">pointwise-conv</figcaption>
</figure>
<h5 id="separable-conv-depthwise-pointwise">Separable Conv = Depthwise + Pointwise</h5>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/13/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="we1k">
      <meta itemprop="description" content="mix 7 mix 8">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Notes&Words">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/13/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-13 09:01:00" itemprop="dateCreated datePublished" datetime="2021-12-13T09:01:00+08:00">2021-12-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">we1k</p>
  <div class="site-description" itemprop="description">mix 7 mix 8</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/we1k" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;we1k" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">we1k</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
